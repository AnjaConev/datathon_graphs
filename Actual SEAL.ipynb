{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual SEAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os.path as osp\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.nn import BCEWithLogitsLoss, Conv1d, MaxPool1d, ModuleList\n",
    "\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MLP, GCNConv, global_sort_pool\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import k_hop_subgraph, to_scipy_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEALDataset(InMemoryDataset):\n",
    "    def __init__(self, dataset, num_hops, filename):\n",
    "        #self.data = dataset[0]\n",
    "        self.num_hops = num_hops\n",
    "        super().__init__('')\n",
    "        #index = ['train', 'val', 'test'].index(split)\n",
    "        self.data, self.slices = torch.load(filename)\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['SEAL_train_data.pt', 'SEAL_val_data.pt', 'SEAL_test_data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        transform = RandomLinkSplit(num_val=0.05, num_test=0.1,\n",
    "                                    is_undirected=True, split_labels=True\n",
    "                                   )\n",
    "        \n",
    "        train_data, val_data, test_data = transform(self.data)\n",
    "\n",
    "        self._max_z = 0\n",
    "\n",
    "        # Collect a list of subgraphs for training, validation and testing:\n",
    "        train_pos_data_list = self.extract_enclosing_subgraphs(\n",
    "            train_data.edge_index, train_data.pos_edge_label_index, 1)\n",
    "        train_neg_data_list = self.extract_enclosing_subgraphs(\n",
    "            train_data.edge_index, train_data.neg_edge_label_index, 0)\n",
    "\n",
    "        val_pos_data_list = self.extract_enclosing_subgraphs(\n",
    "            val_data.edge_index, val_data.pos_edge_label_index, 1)\n",
    "        val_neg_data_list = self.extract_enclosing_subgraphs(\n",
    "            val_data.edge_index, val_data.neg_edge_label_index, 0)\n",
    "\n",
    "        test_pos_data_list = self.extract_enclosing_subgraphs(\n",
    "            test_data.edge_index, test_data.pos_edge_label_index, 1)\n",
    "        test_neg_data_list = self.extract_enclosing_subgraphs(\n",
    "            test_data.edge_index, test_data.neg_edge_label_index, 0)\n",
    "\n",
    "        # Convert node labeling to one-hot features.\n",
    "        for data in chain(train_pos_data_list, train_neg_data_list,\n",
    "                          val_pos_data_list, val_neg_data_list,\n",
    "                          test_pos_data_list, test_neg_data_list):\n",
    "            # We solely learn links from structure, dropping any node features:\n",
    "            data.x = F.one_hot(data.z, self._max_z + 1).to(torch.float)\n",
    "\n",
    "        torch.save(self.collate(train_pos_data_list + train_neg_data_list),\n",
    "                   self.processed_paths[0])\n",
    "        torch.save(self.collate(val_pos_data_list + val_neg_data_list),\n",
    "                   self.processed_paths[1])\n",
    "        torch.save(self.collate(test_pos_data_list + test_neg_data_list),\n",
    "                   self.processed_paths[2])\n",
    "\n",
    "    def extract_enclosing_subgraphs(self, edge_index, edge_label_index, y):\n",
    "        data_list = []\n",
    "        for src, dst in edge_label_index.t().tolist():\n",
    "            sub_nodes, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "                [src, dst], self.num_hops, edge_index, relabel_nodes=True)\n",
    "            src, dst = mapping.tolist()\n",
    "\n",
    "            # Remove target link from the subgraph.\n",
    "            mask1 = (sub_edge_index[0] != src) | (sub_edge_index[1] != dst)\n",
    "            mask2 = (sub_edge_index[0] != dst) | (sub_edge_index[1] != src)\n",
    "            sub_edge_index = sub_edge_index[:, mask1 & mask2]\n",
    "\n",
    "            # Calculate node labeling.\n",
    "            z = self.drnl_node_labeling(sub_edge_index, src, dst,\n",
    "                                        num_nodes=sub_nodes.size(0))\n",
    "\n",
    "            data = Data(x=self.data.x[sub_nodes], z=z,\n",
    "                        edge_index=sub_edge_index, y=y)\n",
    "            data_list.append(data)\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    def drnl_node_labeling(self, edge_index, src, dst, num_nodes=None):\n",
    "        # Double-radius node labeling (DRNL).\n",
    "        src, dst = (dst, src) if src > dst else (src, dst)\n",
    "        adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsr()\n",
    "\n",
    "        idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n",
    "        adj_wo_src = adj[idx, :][:, idx]\n",
    "\n",
    "        idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n",
    "        adj_wo_dst = adj[idx, :][:, idx]\n",
    "\n",
    "        dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True,\n",
    "                                 indices=src)\n",
    "        dist2src = np.insert(dist2src, dst, 0, axis=0)\n",
    "        dist2src = torch.from_numpy(dist2src)\n",
    "\n",
    "        dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True,\n",
    "                                 indices=dst - 1)\n",
    "        dist2dst = np.insert(dist2dst, src, 0, axis=0)\n",
    "        dist2dst = torch.from_numpy(dist2dst)\n",
    "\n",
    "        dist = dist2src + dist2dst\n",
    "        dist_over_2, dist_mod_2 = dist // 2, dist % 2\n",
    "\n",
    "        z = 1 + torch.min(dist2src, dist2dst)\n",
    "        z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)\n",
    "        z[src] = 1.\n",
    "        z[dst] = 1.\n",
    "        z[torch.isnan(z)] = 0.\n",
    "\n",
    "        self._max_z = max(int(z.max()), self._max_z)\n",
    "\n",
    "        return z.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.load('SEAL_train_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(x=[3510985, 45], edge_index=[2, 26085308], y=[120606], z=[3510985]),\n",
       " defaultdict(dict,\n",
       "             {'x': tensor([      0,      48,      86,  ..., 3510977, 3510983, 3510985]),\n",
       "              'edge_index': tensor([       0,      602,     1202,  ..., 26085298, 26085308, 26085308]),\n",
       "              'y': tensor([     0,      1,      2,  ..., 120604, 120605, 120606]),\n",
       "              'z': tensor([      0,      48,      86,  ..., 3510977, 3510983, 3510985])}))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
